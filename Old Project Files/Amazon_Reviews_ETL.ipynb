{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V58rxea0HqSa",
        "outputId": "27f6fab5-9513-4ad7-cffd-0b88c371503f"
      },
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "Unable to find py4j in /opt/homebrew/Caskroom/miniforge/base/bin/pyspark/python, your SPARK_HOME may not be configured correctly",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(spark_python, \u001b[39m\"\u001b[39;49m\u001b[39mlib\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mpy4j-*.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m))[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m    160\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# Start a SparkSession\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfindspark\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m findspark\u001b[39m.\u001b[39;49minit()\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m         py4j \u001b[39m=\u001b[39m glob(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(spark_python, \u001b[39m\"\u001b[39m\u001b[39mlib\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpy4j-*.zip\u001b[39m\u001b[39m\"\u001b[39m))[\u001b[39m0\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnable to find py4j in \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    163\u001b[0m                 spark_python\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m     sys\u001b[39m.\u001b[39mpath[:\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m sys_path \u001b[39m=\u001b[39m [spark_python, py4j]\n\u001b[1;32m    167\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[39m# already imported, no need to patch sys.path\u001b[39;00m\n",
            "\u001b[0;31mException\u001b[0m: Unable to find py4j in /opt/homebrew/Caskroom/miniforge/base/bin/pyspark/python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "spark_version = 'spark-3.4.0'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk/bin/java\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/homebrew/Caskroom/miniforge/base/bin/pyspark\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniforge/base/bin/pyspark\n"
          ]
        }
      ],
      "source": [
        "!which pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xKwTpATHqSe",
        "outputId": "c78b64fb-7329-4ba5-897e-5314ae99df8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-01-26 02:38:44--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1002883 (979K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.16.jar.2’\n",
            "\n",
            "postgresql-42.2.16. 100%[===================>] 979.38K  6.15MB/s    in 0.2s    \n",
            "\n",
            "2023-01-26 02:38:44 (6.15 MB/s) - ‘postgresql-42.2.16.jar.2’ saved [1002883/1002883]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the Postgres driver that will allow Spark to interact with Postgres.\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'pyspark' has no attribute 'sql'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39;49msql\u001b[39m.\u001b[39mSparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mAmazonReviewsETL\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetOrCreate()\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark' has no attribute 'sql'"
          ]
        }
      ],
      "source": [
        "spark = pyspark.sql.SparkSession.builder.appName(\"AmazonReviewsETL\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MMqDAjVS0KN9"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'pyspark' has no attribute 'SparkSession'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# start the spark session\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark \u001b[39m=\u001b[39m pyspark\u001b[39m.\u001b[39;49mSparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m\"\u001b[39m\u001b[39mAmazon-Vine-Analysis\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mgetOrCreate()\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark' has no attribute 'SparkSession'"
          ]
        }
      ],
      "source": [
        "# start the spark session\n",
        "spark = pyspark.SparkSession.builder.appName(\"Amazon-Vine-Analysis\").getOrCreate()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cyBsySGuY-9V"
      },
      "source": [
        "### Load Amazon Data into Spark DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtCmBhQJY-9Z",
        "outputId": "8cb3e69e-a1d6-48d6-8ad5-9597b1008e5c"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'SparkFiles' from 'pyspark' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkFiles\n\u001b[1;32m      2\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Entertainment_v1_00.tsv.gz\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m spark\u001b[39m.\u001b[39msparkContext\u001b[39m.\u001b[39maddFile(url)\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'SparkFiles' from 'pyspark' (unknown location)"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkFiles\n",
        "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Entertainment_v1_00.tsv.gz\"\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "# Read in the Review dataset as a DataFrame\n",
        "df = spark.read.option(\"encoding\", \"UTF-8\").csv(SparkFiles.get(\"\"), sep=\"\\t\", header=True, inferSchema=True)\n",
        "df.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2yUSe55VY-9t"
      },
      "source": [
        "### Create DataFrames to match tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0TESUDRY-90",
        "outputId": "8393f3da-3e7f-4bbb-c913-f8985f175b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|customer_count|\n",
            "+-----------+--------------+\n",
            "|   10142992|             1|\n",
            "|   16457323|             6|\n",
            "|   11935383|             1|\n",
            "|   46277736|             1|\n",
            "|   13671072|             1|\n",
            "|   21453814|             1|\n",
            "|   17684885|             1|\n",
            "|   20415768|             1|\n",
            "|   15212710|             1|\n",
            "|    5220924|             1|\n",
            "|   46253451|             6|\n",
            "|     971908|             1|\n",
            "|   32829933|             1|\n",
            "|   51221518|             1|\n",
            "|   12002637|             2|\n",
            "|   16105308|             1|\n",
            "|     135867|             1|\n",
            "|   47425808|             1|\n",
            "|   43138273|             1|\n",
            "|   16411995|             1|\n",
            "+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the customers_table DataFrame\n",
        "from pyspark.sql.functions import count\n",
        "customers_df = df.groupby(\"customer_id\").agg(count(\"customer_id\")).withColumnRenamed(\"count(customer_id)\", \"customer_count\")\n",
        "customers_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FwXA6UvY-96",
        "outputId": "b0fe3eba-bcff-4837-bbfa-afc027038786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------------------+\n",
            "|product_id|       product_title|\n",
            "+----------+--------------------+\n",
            "|B00N9OT6RM|Upstar 19-Inch 72...|\n",
            "|B007R9RUPU|Kinivo LS210 Port...|\n",
            "|B00U9U9AAM|Samsung J6200, SB...|\n",
            "|B00QCLTOQM|Panasonic TC32A40...|\n",
            "|B007F9XJW0|Sony DVPFX780 7-I...|\n",
            "|B00JA7ZQOY|Minix X7mini Andr...|\n",
            "|B008I641TE|SquareTrade 2-Yea...|\n",
            "|B00EJ5UJZ8|PowerSmart 4200mA...|\n",
            "|B006L8TX94|TiVo Premiere 500...|\n",
            "|B00R8K9ZH4|Fosmon HYBO-DUOC ...|\n",
            "|B001JHJK22|Mediabridge - RCA...|\n",
            "|B00CWEJ5BW|2-Year Electronic...|\n",
            "|B000BSHLLW|Ziotek Purse Lock...|\n",
            "|B0019OJOTE|FAVI 32-Inch 1080...|\n",
            "|B00MOCT3NW|BenQ MH630 1.4A 1...|\n",
            "|B00VWV1O3S|12V 8Ah SLA Batte...|\n",
            "|B00BBAG0DY|LG Electronics LA...|\n",
            "|B00BXF7I8I|Seiki 1080p 60Hz ...|\n",
            "|B0011ZOZ36|Vizio VW26LHDTV20...|\n",
            "|B00Q8DB4YY|Atoah MXIII TV Bo...|\n",
            "+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the products_table DataFrame and drop duplicates. \n",
        "\n",
        "products_df = df.select(['product_id','product_title']).drop_duplicates()\n",
        "products_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkqyCuNQY-9-",
        "outputId": "ef0f5198-73be-4ea8-bc5a-a55bb0a094e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-----------+----------+--------------+-----------+\n",
            "|     review_id|customer_id|product_id|product_parent|review_date|\n",
            "+--------------+-----------+----------+--------------+-----------+\n",
            "| RY01SAV7HZ8QO|     179886|B00NTI0CQ2|     667358431| 2015-08-31|\n",
            "|R1XX8SDGJ4MZ4L|   37293769|B00BUCLVZU|     621695622| 2015-08-31|\n",
            "|R149Q3B5L33NN5|    8332121|B00RBX9D5W|     143071132| 2015-08-31|\n",
            "|R2ZVD69Z6KPJ4O|   47054962|B00UJ3IULO|     567816707| 2015-08-31|\n",
            "|R1DIKG2G33ZLNP|   23413911|B0037UCTXG|     909557698| 2015-08-31|\n",
            "|R3L6FGKAW0EYFI|    4417771|B004N866SU|     414565179| 2015-08-31|\n",
            "| RAO0QZH5VC6VI|   47900707|B00JE6AOJS|     798450889| 2015-08-31|\n",
            "|R25IK0UAHWNB22|   34112894|B00COL0B7A|     777554234| 2015-08-31|\n",
            "|R2A9IHKZMTMAL1|   20691979|B00QHLSKOE|     885228855| 2015-08-31|\n",
            "| R5XVKTHL6SITI|   25983343|B00UNL2MUW|     164482798| 2015-08-31|\n",
            "|R2QZZOSTDDY1IE|   35816068|B00RIC9JB4|     184834831| 2015-08-31|\n",
            "|R38CUDCFPSNYTD|   10628020|B00HPMCO6O|     444378461| 2015-08-31|\n",
            "| RM6ZR6NH052YH|    9059625|B004QGXWS6|     770226547| 2015-08-31|\n",
            "| RUQK5N4WH8UN8|    2681147|B00FO12XY6|     448806082| 2015-08-31|\n",
            "|R21LWSBQWWJYZ3|   33449922|B00BD7UVO4|     374427271| 2015-08-31|\n",
            "| R8W5S53RQ2DF7|   43069144|B00TRQPEYK|     614207013| 2015-08-31|\n",
            "|R3ENME3JQBWXZS|   46780686|B005STXQG8|     689442799| 2015-08-31|\n",
            "|R3URL5K7DHHYK7|   49037595|B00BEL11RA|     910670994| 2015-08-31|\n",
            "|R22YISZKS35YZX|   27868511|B00QHLSKOE|     885228855| 2015-08-31|\n",
            "|R3IIOLWHWC297U|    3004043|B00MWCJ8VQ|     946374680| 2015-08-31|\n",
            "+--------------+-----------+----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the review_id_table DataFrame. \n",
        "# Convert the 'review_date' column to a date datatype with to_date(\"review_date\", 'yyyy-MM-dd').alias(\"review_date\")\n",
        "from pyspark.sql.functions import to_date\n",
        "review_id_df = df.select(\n",
        "    ['review_id',\n",
        "     \"customer_id\",\n",
        "     \"product_id\",\n",
        "     \"product_parent\",\n",
        "     to_date(\"review_date\", 'yyyy-MM-dd').alias(\"review_date\")\n",
        "     ])\n",
        "\n",
        "review_id_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzMmkdKmY--D",
        "outputId": "e8a5636b-f67f-4bec-a4fa-8fd58667fbf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+-----------+-------------+-----------+----+-----------------+\n",
            "|     review_id|star_rating|helpful_votes|total_votes|vine|verified_purchase|\n",
            "+--------------+-----------+-------------+-----------+----+-----------------+\n",
            "| RY01SAV7HZ8QO|          4|            0|          0|   N|                Y|\n",
            "|R1XX8SDGJ4MZ4L|          5|            0|          0|   N|                N|\n",
            "|R149Q3B5L33NN5|          5|            0|          0|   N|                Y|\n",
            "|R2ZVD69Z6KPJ4O|          1|            0|          2|   N|                Y|\n",
            "|R1DIKG2G33ZLNP|          4|            0|          0|   N|                Y|\n",
            "|R3L6FGKAW0EYFI|          1|            1|          1|   N|                N|\n",
            "| RAO0QZH5VC6VI|          1|            0|          0|   N|                Y|\n",
            "|R25IK0UAHWNB22|          3|            0|          0|   N|                Y|\n",
            "|R2A9IHKZMTMAL1|          5|            1|          2|   N|                Y|\n",
            "| R5XVKTHL6SITI|          5|            0|          0|   N|                Y|\n",
            "|R2QZZOSTDDY1IE|          3|            3|          6|   N|                Y|\n",
            "|R38CUDCFPSNYTD|          5|            0|          0|   N|                Y|\n",
            "| RM6ZR6NH052YH|          3|            1|          2|   N|                Y|\n",
            "| RUQK5N4WH8UN8|          5|            0|          0|   N|                Y|\n",
            "|R21LWSBQWWJYZ3|          5|            0|          0|   N|                Y|\n",
            "| R8W5S53RQ2DF7|          5|            0|          0|   N|                Y|\n",
            "|R3ENME3JQBWXZS|          5|            0|          1|   N|                Y|\n",
            "|R3URL5K7DHHYK7|          5|            1|          1|   N|                Y|\n",
            "|R22YISZKS35YZX|          5|            2|          3|   N|                Y|\n",
            "|R3IIOLWHWC297U|          5|            0|          0|   N|                Y|\n",
            "+--------------+-----------+-------------+-----------+----+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the vine_table. DataFrame\n",
        "vine_df = df.select([\n",
        "    'review_id',\n",
        "    'star_rating',\n",
        "    'helpful_votes',\n",
        "    'total_votes',\n",
        "    'vine',\n",
        "    'verified_purchase'\n",
        "    ])\n",
        "vine_df.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jITZhLkmY--J"
      },
      "source": [
        "### Connect to the AWS RDS instance and write each DataFrame to its table. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jiUvs1aY--L"
      },
      "outputs": [],
      "source": [
        "# Configure settings for RDS\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://dataviz.cozcjayx8uch.us-west-2.rds.amazonaws.com:5432/Challenge_Analysis\"\n",
        "config = {\"user\":\"user_name\", \n",
        "          \"password\": \"******\", \n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2zgZ-aKY--Q"
      },
      "outputs": [],
      "source": [
        "# Write review_id_df to table in RDS\n",
        "review_id_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m3yzn-LY--U"
      },
      "outputs": [],
      "source": [
        "# Write products_df to table in RDS\n",
        "products_df.write.jdbc(url=jdbc_url, table='products_table', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbXri15fY--Z"
      },
      "outputs": [],
      "source": [
        "# Write customers_df to table in RDS\n",
        "customers_df.write.jdbc(url=jdbc_url, table='customers_table', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdQknSHLY--e"
      },
      "outputs": [],
      "source": [
        "# Write vine_df to table in RDS\n",
        "vine_df.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gihMPIVu_oLO"
      },
      "outputs": [],
      "source": [
        "# End the session\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
